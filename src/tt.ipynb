{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1305, 2.2971, 3.2298]),\n",
       " tensor([[0.1305, 0.0079, 0.1849],\n",
       "         [0.5490, 2.2971, 0.9173],\n",
       "         [3.7014, 3.2826, 3.2298]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from args import args_define\n",
    "from typing import List, Tuple, Dict\n",
    "import clip\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import CIRRDataset, CIRCODataset\n",
    "from utils import extract_image_features, device, collate_fn, PROJECT_ROOT, targetpad_transform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-g-14', device=device, pretrained='laion2b_s34b_b88k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-g-14')\n",
    "args = args_define.args \n",
    "\n",
    "####初始化检索图片特征\n",
    "classic_test_dataset = CIRRDataset(args.dataset_path, 'test1', 'classic', clip_preprocess)\n",
    "index_features, index_names, _ = extract_image_features(classic_test_dataset, clip_model)\n",
    "index_features = index_features.to(device)\n",
    "index_features = F.normalize(index_features, dim=-1).float()\n",
    "\n",
    "###初始化参考文本特征\n",
    "reference_names = \"test1-1051-1-img1\"\n",
    "text = [\"husky puppy, wooden floor, towels.\",\"husky puppy, wooden floor, towels.\",\"husky puppy, wooden floor, towels.\"]\n",
    "text_features_list = []\n",
    "for cap in text:\n",
    "    tokenized_input_captions = tokenizer(cap, context_length=77).to(device)\n",
    "    text_features = clip_model.encode_text(tokenized_input_captions)\n",
    "    text_features_list.append(text_features)\n",
    "text_features_list = torch.vstack(text_features_list)\n",
    "text_features = torch.mean(text_features_list, dim=0)\n",
    "predicted_features = F.normalize(text_features).float()\n",
    "\n",
    "#### 计算相似度\n",
    "distances = 1 - predicted_features @ index_features.T\n",
    "sorted_indices = torch.argsort(distances, dim=-1).cpu()\n",
    "sorted_index_names = np.array(index_names)[sorted_indices]\n",
    "\n",
    "\n",
    "# Delete the reference image from the results\n",
    "reference_mask = torch.tensor(\n",
    "        sorted_index_names != np.repeat(np.array(reference_names), len(index_names)).reshape(len(sorted_index_names),\n",
    "                                                                                             -1))\n",
    "sorted_index_names = sorted_index_names[reference_mask].reshape(sorted_index_names.shape[0],\n",
    "                                                                    sorted_index_names.shape[1] - 1)\n",
    "sorted_index_names = sorted_index_names.tolist()\n",
    "print(sorted_index_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1267, -0.1935])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 a 是一个形状为 (20, 2) 的张量\n",
    "a = torch.randn(20, 2)\n",
    "\n",
    "# 计算每一列的平均值\n",
    "column_means = torch.mean(a, dim=0)\n",
    "\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'context_length': 77} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Text: the image shows a group of wolves playing in the snow , with a wooden wall in the background . the wolves appear to be engaged in a fierce battle , with one of them lying on the ground and the others surrounding it .\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not BatchEncoding",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# # 将遮掩后的文本编码\u001b[39;00m\n\u001b[0;32m     32\u001b[0m tokenized_input_captions \u001b[38;5;241m=\u001b[39m tokenizer(masked_text, context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 33\u001b[0m text_features \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mencode_text(tokenized_input_captions)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# # 图像特征获取（假设有一个预训练的图像编码器）\u001b[39;00m\n\u001b[0;32m     36\u001b[0m reference_images \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../CIRR/test1/test1-147-1-img1.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\clip\\model.py:344\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 344\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(text)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    347\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not BatchEncoding"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import clip\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# 初始化BERT编码器和分词器\n",
    "clip_model,_ = clip.load('ViT-L/14', device=device, jit=False)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 输入文本\n",
    "text = \"The image shows a group of wolves playing in the snow, with a wooden wall in the background. The wolves appear to be engaged in a fierce battle, with one of them lying on the ground and the others surrounding it.\"\n",
    "\n",
    "# 将文本随机遮掩部分词语\n",
    "def mask_text(text, mask_token='[MASK]', mask_prob=0):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    masked_tokens = [\n",
    "        (mask_token if random.random() < mask_prob else token)\n",
    "        for token in tokens\n",
    "    ]\n",
    "    masked_text = ' '.join(masked_tokens)\n",
    "    return masked_text\n",
    "\n",
    "# 生成遮掩后的文本\n",
    "masked_text = mask_text(text)\n",
    "print(\"Masked Text:\", masked_text)\n",
    "\n",
    "# # 将遮掩后的文本编码\n",
    "tokenized_input_captions = tokenizer(masked_text, context_length=77).to(device)\n",
    "text_features = clip_model.encode_text(tokenized_input_captions)\n",
    "\n",
    "# # 图像特征获取（假设有一个预训练的图像编码器）\n",
    "reference_images = Image.open('../CIRR/test1/test1-147-1-img1.png')\n",
    "image_features = clip_model.encode_image(reference_images)\n",
    "\n",
    "similarity = (text_features @ image_features.T)\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "# # 获取文本特征\n",
    "# text_features = outputs.last_hidden_state\n",
    "\n",
    "# # 图像特征获取（假设有一个预训练的图像编码器）\n",
    "# # image_features = image_encoder(image_input)\n",
    "\n",
    "# # 匹配步骤可以通过计算相似度来完成，如余弦相似度或其他对比损失\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
